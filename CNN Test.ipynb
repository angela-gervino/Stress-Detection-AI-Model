{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/dbfs/FileStore/tables/Models/vX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Option 1): Make Predictions Using Folders of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders follow the structure where there is 1 base folder which contains 1 folder for each label\n",
    "\n",
    "emotions = [\"Happy\", \"Surprise\", \"Disgust\", \"Neutral\", \"Angry\", \"Fear\", \"Sad\"]\n",
    "\n",
    "# Get paths to all of the images to predict on\n",
    "file_paths = []\n",
    "for emotion in emotions:\n",
    "    path = f\"/dbfs/FileStore/tables/Test Set Cropped/{emotion}/\"\n",
    "    files = glob.glob(path + \"*.jpg\", recursive=True)\n",
    "    file_paths.extend(files)\n",
    "\n",
    "xdata = []\n",
    "ydata = []\n",
    "# Get pixels with corresponding labels for the images\n",
    "for file_path in file_paths:\n",
    "    image = Image.open(file_path).convert('L').getdata()\n",
    "    resized = image.resize((48, 48))\n",
    "    xdata.append(np.array(resized).reshape((48, 48, 1)))\n",
    "    ydata.append(file_path.split('/')[5])\n",
    "\n",
    "xtest = np.array(xdata).astype('float64')\n",
    "ytest = np.array(ydata)\n",
    "\n",
    "# Preprocess data the same way as done to the training data\n",
    "xtest -= np.mean(xtest, axis=0)\n",
    "xtest /= np.std(xtest, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = model.predict(xtest) \n",
    "\n",
    "predicted = []\n",
    "actual = []\n",
    "emotion_to_int = {\"Angry\":0, \"Disgust\":1, \"Fear\":2, \"Happy\":3, \"Sad\":4, \"Surprise\":5, \"Neutral\":6}\n",
    "\n",
    "# Get a single prediction for each list of probabilities\n",
    "for i in range(len(predicted_probabilities)):\n",
    "    predicted.append(np.argmax(predicted_probabilities[i]))\n",
    "    actual.append(emotion_to_int[ytest[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Option 2): Make Predictions Using Data Stored as a .npy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "xtest = np.load('/dbfs/FileStore/tables/npy/xtest.npy')\n",
    "ytest = np.load('/dbfs/FileStore/tables/npy/ytest.npy').tolist()\n",
    "\n",
    "# Preprocess data the same way as done to the training data\n",
    "xtest -= np.mean(xtest, axis=0)\n",
    "xtest /= np.std(xtest, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = model.predict(xtest)\n",
    "\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for i in range(len(predicted_probabilities)):\n",
    "    predicted.append(np.argmax(predicted_probabilities[i]))\n",
    "    actual.append(ytest[i].index(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix for the test data\n",
    "\n",
    "labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "cm = confusion_matrix(actual, predicted)\n",
    "title='Confusion Matrix'\n",
    "    \n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "fmt = 'd'\n",
    "thresh = cm.max() / 2. \n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt), \n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1-Score:   {dict( zip( labels, f1_score(actual, predicted, average=None) ) )}\")\n",
    "print(f\"Accuracy:   {accuracy_score(actual, predicted)}\")\n",
    "print(f\"Precision:  {dict( zip( labels, precision_score(actual, predicted, average=None) ) )}\")\n",
    "print(f\"Recall:     {dict( zip( labels, recall_score(actual, predicted, average=None) ) )}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
