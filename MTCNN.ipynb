{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\agervino\\.conda\\envs\\project\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Face Images.csv\").sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, box):\n",
    "    # Input: image: np array of pixels (image) to crop\n",
    "    #        box: a dictionary returned by detect_faces() containing coordinates/width/height of the box outlining the detected face\n",
    "    # Output: an np array of pixels representing the cropped image\n",
    "    x, y, width, height = box[0], box[1], box[2], box[3] # x,y,w,h of the box containing a detected face\n",
    "    return image[y:y+height, x:x+width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image):\n",
    "    # Input: image: an np array of pixels\n",
    "    # Output: an np array representing the input image resized to 224x224 px\n",
    "    size = (224, 224)\n",
    "    return np.array(Image.fromarray(np.squeeze(image, axis=2)).resize(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each image in the df, use the mtcnn to detect the face, then crop the image to just the face portion, and then resize to a constant size of 224x224 px\n",
    "\n",
    "images = []\n",
    "detector = MTCNN()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    original_image = np.fromstring(row[\"pixels\"], dtype=int, sep=\" \").reshape(row[\"height\"], row[\"width\"], 1).astype(\"float32\")\n",
    "    img = np.tile(original_image, [1,1,3]) # this MTCNN library only works with colour images (imgs with 3 channels), but grayscale images have only 1 channel. This pads the img to make the image have 3 channels\n",
    "    output = detector.detect_faces(img)\n",
    "    if len(output) == 1: # 1 face detected\n",
    "        resized_image = resize_image(crop_image(original_image, output[0][\"box\"]))\n",
    "        images.append([row[\"emotion\"], resized_image])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
