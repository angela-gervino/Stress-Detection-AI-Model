{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code used in my presentation/demo for the AI group on 22/3/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, box):\n",
    "    x, y, width, height = box[0], box[1], box[2], box[3]\n",
    "    return image[y:y+height, x:x+width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_rgb_image(image, size=(48, 48)):\n",
    "    return np.expand_dims(np.array((Image.fromarray(image).convert('L')).resize(size)), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "I’ve already done the step where we extract the frames from the video, so we’re going to be starting with a folder that has all of those frames in it (the Frames folder below).\n",
    "Here we’re getting the paths to all of the frames from the video and just storing them.\n",
    "\"\"\"\n",
    "\n",
    "file_paths = glob.glob(\"/dbfs/FileStore/tables/Frames/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Here we can see some of the different frames from the video. \n",
    "\"\"\"\n",
    "\n",
    "display(Image.open(file_paths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Then for each frame, take the pixels and store those in a list.\n",
    "\"\"\"\n",
    "\n",
    "# Store the pixels for each frame in a list\n",
    "pixels = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    pixels.append(np.asarray(Image.open(file_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Then once we have all those pixels we can move on to the MTCNN step. \n",
    "For each image we’ll detect the face. Then if exactly 1 face was detected, crop the image, resize it, convert to grayscale (since the model was trained on grayscale images), and then save the pixels of the cropped image. \n",
    "I chose to discard images where more than one face was detected because at this point there is no way to determine which face the image should be cropped to without going in and manually checking it myself \n",
    "since I haven’t implemented any sort of remembering previous faces and matching them to each other.\n",
    "\"\"\"\n",
    "\n",
    "# Use MTCNN to detect faces and then crop the images\n",
    "detector = MTCNN()\n",
    "cropped_images = []\n",
    "\n",
    "for img in pixels:\n",
    "    output = detector.detect_faces(img)\n",
    "    if (len(output) == 1):\n",
    "        resized_image = resize_rgb_image(crop_image(img, output[0][\"box\"]))\n",
    "        cropped_images.append(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Standardizing the data the same way as was done to the training data and then turning into a format that can be passed to the model.\n",
    "\"\"\"\n",
    "\n",
    "x = cropped_images\n",
    "x -= np.mean(x, axis=0)\n",
    "x /= np.std(x, axis=0)\n",
    "\n",
    "x_test = tf.convert_to_tensor(x, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Then we’ll load in the model and predict on the frames.\n",
    "\"\"\"\n",
    "\n",
    "# Load in the pretrained model\n",
    "model = tf.keras.models.load_model('/dbfs/FileStore/tables/Models/v8')\n",
    "\n",
    "# Predict on all of the frames\n",
    "probabilities = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What we actually get back as a prediction for each frame is a list of probabilities, one for each of the facial expression classes in this order: [Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral]\n",
    "So, for frame 0 this is the output here. \n",
    "The probability at index X is the probability that the image belongs/should be classified as class X\n",
    "\"\"\"\n",
    "\n",
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Turn all of the lists of probabilities into an integer representing the class that it’s predicting. \n",
    "That class will be the one that had the highest probability.\n",
    "\"\"\"\n",
    "\n",
    "# For each probability distribution, determine what the prediction is\n",
    "predicted = []\n",
    "\n",
    "for pred in probabilities:\n",
    "    predicted.append(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then to get the predicted emotion for the whole video, I took that to be the class that was predicted the most over all of the frames.\n",
    "\"\"\"\n",
    "\n",
    "# Find most frequent prediction\n",
    "expressions = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"} # map integers to the corresponding emotion\n",
    "print(f\"Predicted emotion: {expressions[max(set(predicted), key=predicted.count)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We’re saying that somebody is stressed if over 2/3 of the frames from the video fall into the stress-related classes. \n",
    "So down here we’re counting how many of the frames fall into those classes and how many frames we have total, and using that to make the prediction of whether the person is stressed or not.\n",
    "\"\"\"\n",
    "\n",
    "# Predict if stressed or not\n",
    "num_stressed = 0\n",
    "total = 0\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] in {0, 2, 4}:\n",
    "        num_stressed += 1\n",
    "    total += 1\n",
    "\n",
    "if num_stressed > 2*total/3:\n",
    "    print(\"Prediction: Stressed\")\n",
    "else:\n",
    "    print(\"Prediction: Not stressed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
