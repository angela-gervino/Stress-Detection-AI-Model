{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizer, loss, metrics):\n",
    "    input_ = tf.keras.layers.Input(shape=(224, 224, 1), name=\"INPUT\")\n",
    "    #normalization = tf.keras.layers.Rescaling(1./255)(input_)\n",
    "    n1 = tf.keras.layers.BatchNormalization()(input_)\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(7, 7), strides=1, activation=\"relu\", padding=\"same\", name=\"CONV1\", data_format='channels_last', kernel_regularizer=tf.keras.regularizers.l2(0.01))(n1) \n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, name=\"POOL1\")(conv1) \n",
    "    dropout0 = tf.keras.layers.Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(7, 7), strides=1, activation=\"relu\", padding=\"same\", name=\"CONV2\")(dropout0)\n",
    "    n2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    concat1 = tf.keras.layers.Concatenate()([pool1, n2])\n",
    "    \n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, name=\"POOL2\")(concat1)\n",
    "    dropout01 = tf.keras.layers.Dropout(0.5)(pool2)\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5, 5), strides=1, activation=\"relu\", padding=\"same\", name=\"CONV3\")(dropout01)\n",
    "    n3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    concat2 = tf.keras.layers.Concatenate()([pool2, n3])\n",
    "    \n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, name=\"POOL3\")(concat2)\n",
    "    dropout02 = tf.keras.layers.Dropout(0.5)(pool3)\n",
    "    conv4 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5, 5), strides=1, activation=\"relu\", padding=\"same\", name=\"CONV4\")(dropout02)\n",
    "    n4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    concat3 = tf.keras.layers.Concatenate()([pool3, n4])\n",
    "    \n",
    "    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, name=\"POOL4\")(concat3)\n",
    "    dropout03 = tf.keras.layers.Dropout(0.5)(pool4)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()(dropout03)\n",
    "    \n",
    "    fc1 = tf.keras.layers.Dense(4096, activation='relu')(flatten)\n",
    "    fc2 = tf.keras.layers.Dense(2048, activation='relu')(fc1)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(7, activation='softmax')(fc2)\n",
    "    \n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_],\n",
    "        outputs=[output]\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = pathlib.Path(\"/dbfs/FileStore/tables/Train\").with_suffix('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = (224, 224)\n",
    "val_split = 0.1\n",
    "batch_size = 64\n",
    "seed = 13\n",
    "mode = \"grayscale\"\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=val_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(im_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=mode\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=val_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(im_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(optimizer=tf.keras.optimizers.Adam(), \n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(val_ds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, training_loss, 'r-')\n",
    "plt.plot(epochs, validation_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epochs vs Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, training_accuracy, 'r-')\n",
    "plt.plot(epochs, validation_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Epochs vs Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
